{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6380645,"sourceType":"datasetVersion","datasetId":3677153}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Lab 3 Manual: Basics of Neural Networks with Tensorflow*","metadata":{}},{"cell_type":"markdown","source":"# **Libraries import and Data Cleaning**\n\n**pandas:** the first library we learned, used for loading and handling datasets.\n\n**numpy:** helps with math operations and working with arrays.\n\n**tensorflow / keras:** used to build and train our neural network model.\n\n**train_test_split:** splits the data into training and testing sets.\n\n**StandardScaler:** prepares the data by scaling the features for better model performance.","metadata":{}},{"cell_type":"code","source":"#arsenal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import Sequential, Input\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.metrics import Precision, Recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:24:58.642691Z","iopub.execute_input":"2025-09-27T09:24:58.642975Z","iopub.status.idle":"2025-09-27T09:25:19.526347Z","shell.execute_reply.started":"2025-09-27T09:24:58.642952Z","shell.execute_reply":"2025-09-27T09:25:19.525257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next step is to load and prepare the data. You can refer to Lab Manual 1 for more information about Data Cleaning.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fantasy-football/master_team_list.csv')\n# Basic cleaning\ndf['Age'] = df['Age'].fillna(df['Age'].median())\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\ndf['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\ndf = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n# Drop irrelevant columns\ndf = df.drop(columns=['Cabin', 'Name', 'Ticket'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.528181Z","iopub.execute_input":"2025-09-27T09:25:19.528754Z","iopub.status.idle":"2025-09-27T09:25:19.574764Z","shell.execute_reply.started":"2025-09-27T09:25:19.528726Z","shell.execute_reply":"2025-09-27T09:25:19.573924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's take a look at the data after applying the preprocessing techniques","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.575562Z","iopub.execute_input":"2025-09-27T09:25:19.575864Z","iopub.status.idle":"2025-09-27T09:25:19.603255Z","shell.execute_reply.started":"2025-09-27T09:25:19.575844Z","shell.execute_reply":"2025-09-27T09:25:19.602235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Defining Features and Target** \nBefore training a neural network model, we need to extract the features and the target (ground truth) from the dataset, depending on the task we want the model to learn.\n\n**Features (X):** We select the columns Pclass, Sex, Age, Fare, SibSp, and Parch from the dataset. These variables will be used as inputs to the model.\n\n**Target (y):** We choose the Survived column as the target variable, which represents the outcome we want the model to predict.","metadata":{}},{"cell_type":"code","source":"# Feature and target\nX = df[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch']]\ny = df['Survived']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.605569Z","iopub.execute_input":"2025-09-27T09:25:19.605936Z","iopub.status.idle":"2025-09-27T09:25:19.611976Z","shell.execute_reply.started":"2025-09-27T09:25:19.605912Z","shell.execute_reply":"2025-09-27T09:25:19.611042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the first 5 rows of features\nprint(\"Features (X):\")\nprint(X.head())\n\n# Show the first 5 rows of target values\nprint(\"\\nTarget (y):\")\nprint(y.head())\n\n#Show the dimensions of both the Features and Targets dataframes\nprint(\"X and y dimensions:\",X.shape,y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.612934Z","iopub.execute_input":"2025-09-27T09:25:19.613206Z","iopub.status.idle":"2025-09-27T09:25:19.633977Z","shell.execute_reply.started":"2025-09-27T09:25:19.613186Z","shell.execute_reply":"2025-09-27T09:25:19.633027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Features Scaling**\n\nWe usually scale features before training machine learning models because raw features can have very different ranges and units, which may cause problems.\n\nFor example, in the Titanic dataset:\n\n* Age ranges from about 0.1–80 years\n\n* Fare ranges from about 20–500 dollars\n\nWithout scaling, algorithms that rely on distances or weights may treat Fare as more important than Age simply because the numbers are larger.\n\nStandardization solves this by transforming each feature. We don't need to do that with the target labels, only the features.","metadata":{}},{"cell_type":"code","source":"# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Print first 5 rows of scaled features\nprint(\"Scaled Features (first 5 rows):\")\nprint(X_scaled[:5])\n\nprint(\"Shape of 1 row:\")\nprint(X_scaled[0].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.634955Z","iopub.execute_input":"2025-09-27T09:25:19.635669Z","iopub.status.idle":"2025-09-27T09:25:19.657506Z","shell.execute_reply.started":"2025-09-27T09:25:19.635638Z","shell.execute_reply":"2025-09-27T09:25:19.656425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.658597Z","iopub.execute_input":"2025-09-27T09:25:19.659001Z","iopub.status.idle":"2025-09-27T09:25:19.677814Z","shell.execute_reply.started":"2025-09-27T09:25:19.658978Z","shell.execute_reply":"2025-09-27T09:25:19.676945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Explore the size and dimension of the train and test data after splitting:\nprint(\"Train data shape:\",X_train.shape, \"Train Label shape\", y_train.shape)\nprint(\"Test data shape:\",X_test.shape, \"Test Label shape\", y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.678691Z","iopub.execute_input":"2025-09-27T09:25:19.678935Z","iopub.status.idle":"2025-09-27T09:25:19.697152Z","shell.execute_reply.started":"2025-09-27T09:25:19.678915Z","shell.execute_reply":"2025-09-27T09:25:19.696081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Build a Feedforward Neural Network model**","metadata":{}},{"cell_type":"markdown","source":"**Input Layer:** Input(shape=(X_train.shape[1],)) creates the input layer with one neuron for each feature in the training data.\n\n**First Hidden Layer:** A dense (fully connected) layer with 16 neurons. Each neuron learns from all the inputs of the previous layer.\n\n**ReLU activation:** Outputs the input value if it’s positive, or 0 if it’s negative.\n\n**Fully Connected Network:** Every neuron in one layer connects to all neurons in the next, each with its own weight, enabling the model to learn complex feature relationships.","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Input(shape=(X_train.shape[1],)),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # binary classification\n    ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"model.summary() displays a layer-by-layer summary of the neural network, including the number of parameters and how data flows through the model.\n\nFrom the model's summary, we can see that the model parameters, it has a total of 257 parameters, all of which are trainable, meaning they will be updated during training. There are no non-trainable parameters, so every weight and bias in the network is being learned from the data.","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.804152Z","iopub.execute_input":"2025-09-27T09:25:19.804415Z","iopub.status.idle":"2025-09-27T09:25:19.821471Z","shell.execute_reply.started":"2025-09-27T09:25:19.804395Z","shell.execute_reply":"2025-09-27T09:25:19.820313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[Extra] If you are curious how our model reached 257 parameters, here's a breakdown:\n| Connection                   | Weights Calculation | Biases | Total Params |\n| ---------------------------- | ------------------- | ------ | ------------ |\n| Input (6) → Dense(16, ReLU)  | (6 * 16 = 96)  | 16     | 112          |\n| Dense(16) → Dense(8, ReLU)   | (16 * 8 = 128) | 8      | 136          |\n| Dense(8) → Dense(1, Sigmoid) | (8 * 1 = 8)    | 1      | 9            |\n| **Total**                    | —                   | —      | **257**      |\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Compiling the Model**\n\nBefore training, we need to compile the model by specifying how it will learn:\n\nOptimizer = 'adam': An efficient variant of gradient descent that adapts the learning rate during training.\n\nLoss = 'binary_crossentropy': We chose this because our Titanic task has only two possible outputs, either **survived** or **not survived**. This loss function is designed for binary classification problems.\n\nAccuracy, Precision, and Recall: are evaluation metrics that track how often the model’s predictions match the true labels, giving an easy-to-understand measure of performance.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy',\n                       Precision(name='precision'),\n                       Recall(name='recall')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.822424Z","iopub.execute_input":"2025-09-27T09:25:19.82279Z","iopub.status.idle":"2025-09-27T09:25:19.850652Z","shell.execute_reply.started":"2025-09-27T09:25:19.822761Z","shell.execute_reply":"2025-09-27T09:25:19.849852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train the Neural Network Model**","metadata":{}},{"cell_type":"markdown","source":"**Validation Split:** Part of the training data is set aside (e.g., 80% train, 20% val) to monitor learning.\n\n**Epoch:** One full pass through training data. More epochs help learning, but too many → overfitting.\n\n**Loss** measures how far predictions are from true labels. We want this value to be as small as possible, ideally approaching 0.\n\n**Accuracy** is the percentage of correct predictions. Values range from 0 to 1 (or 0% to 100%), and higher is better.\n\nSame for both **precision** and **recall.** Values range from 0 to 1 (or 0% to 100%), and higher is better.","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:19.851476Z","iopub.execute_input":"2025-09-27T09:25:19.85176Z","iopub.status.idle":"2025-09-27T09:25:26.191094Z","shell.execute_reply.started":"2025-09-27T09:25:19.851741Z","shell.execute_reply":"2025-09-27T09:25:26.190215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To avoid overfitting when training with many epochs, we use **early stopping**, which automatically stops training once the validation performance stops improving.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=50,\n                    validation_split=0.2,\n                    verbose=1,\n                    callbacks=[early_stop])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:41:26.726401Z","iopub.execute_input":"2025-09-27T09:41:26.72681Z","iopub.status.idle":"2025-09-27T09:41:27.488202Z","shell.execute_reply.started":"2025-09-27T09:41:26.726786Z","shell.execute_reply":"2025-09-27T09:41:27.487294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Rather than feeding the entire dataset through the network in a single step, the training data is divided into smaller subsets called **batches**. The model processes one batch at a time and updates its weights after each batch","metadata":{}},{"cell_type":"code","source":"history2 = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:26.1923Z","iopub.execute_input":"2025-09-27T09:25:26.192552Z","iopub.status.idle":"2025-09-27T09:25:29.277693Z","shell.execute_reply.started":"2025-09-27T09:25:26.192532Z","shell.execute_reply":"2025-09-27T09:25:29.27668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Epochs**\n\nOne epoch = full pass through training data (all batches).\n\nToo few: underfitting (model hasn’t learned enough).\n\nToo many: overfitting + wasted compute.\n\nRecommendation: start with moderate epochs (e.g., 30–100) and use early stopping.\n\n**Batch Size**\n\nDefinition: number of samples before a weight update.\n\nSmall (16–32): more updates, better generalization, slower per epoch.\n\nLarge (128–256): fewer updates, faster per epoch, higher memory use, risk of weaker generalization.\n\nCommon defaults: 32 or 64.","metadata":{}},{"cell_type":"markdown","source":"# **Visualize the training progress.**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:25:29.279537Z","iopub.execute_input":"2025-09-27T09:25:29.27987Z","iopub.status.idle":"2025-09-27T09:25:29.28439Z","shell.execute_reply.started":"2025-09-27T09:25:29.279847Z","shell.execute_reply":"2025-09-27T09:25:29.283577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot accuracy, precision, and recall from training history\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\n\nplt.plot(history.history['precision'], label='Train Precision')\nplt.plot(history.history['val_precision'], label='Val Precision')\n\nplt.plot(history.history['recall'], label='Train Recall')\nplt.plot(history.history['val_recall'], label='Val Recall')\n\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.title('Training Progress (Accuracy, Precision, Recall)')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:28:07.785082Z","iopub.execute_input":"2025-09-27T09:28:07.785567Z","iopub.status.idle":"2025-09-27T09:28:08.036494Z","shell.execute_reply.started":"2025-09-27T09:28:07.785535Z","shell.execute_reply":"2025-09-27T09:28:08.035478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Evaluate on the test set.**","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\nprint(\"Test Precision:\", test_precision)\nprint(\"Test Recall:\", test_recall)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:29:20.193884Z","iopub.execute_input":"2025-09-27T09:29:20.194228Z","iopub.status.idle":"2025-09-27T09:29:20.323098Z","shell.execute_reply.started":"2025-09-27T09:29:20.194203Z","shell.execute_reply":"2025-09-27T09:29:20.32221Z"}},"outputs":[],"execution_count":null}]}